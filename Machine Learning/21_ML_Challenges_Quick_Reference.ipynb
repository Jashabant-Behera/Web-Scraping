{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Challenges & Problems - Quick Reference Guide\n",
    "\n",
    "## One-Page Summary for Every Challenge\n",
    "\n",
    "---\n",
    "\n",
    "## 1. DATA COLLECTION\n",
    "\n",
    "**Problem:** Getting data is hard (expensive, legal issues, scraping blocked)\n",
    "\n",
    "**Web Scraping Issues:**\n",
    "- Legal: ToS violations, GDPR/CCPA compliance\n",
    "- Technical: CAPTCHA, IP blocking, JavaScript rendering\n",
    "- Ethical: Copyright, rate limiting\n",
    "\n",
    "**API Limitations:**\n",
    "- Rate limits: Only X requests per hour\n",
    "- Quota limits: Only Y requests per month\n",
    "- Unstable: APIs change format or disappear\n",
    "\n",
    "**Solutions:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use APIs responsibly\n",
    "import time\n",
    "response = requests.get(url, headers={'Authorization': f'Bearer {token}'})\n",
    "time.sleep(2)  # Respectful delay\n",
    "\n",
    "# Use Selenium for dynamic content\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(url)\n",
    "element = WebDriverWait(driver, 10).until(EC.presence_of_element_located((By.CLASS_NAME, \"data\")))\n",
    "\n",
    "# Better: Use free datasets\n",
    "from sklearn.datasets import load_iris\n",
    "from tensorflow import keras\n",
    "keras.datasets.mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cost:** $100 - $50,000+ depending on method\n",
    "\n",
    "---\n",
    "\n",
    "## 2. INSUFFICIENT / LABELED DATA\n",
    "\n",
    "**Problem:** Need lots of labeled data, but expensive/time-consuming to label\n",
    "\n",
    "**Costs:**\n",
    "- Manual labeling: $0.10 - $10+ per sample\n",
    "- 100,000 samples: $10,000 - $500,000\n",
    "- Medical imaging: Even more expensive\n",
    "\n",
    "**Solutions:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Augmentation: Make more samples from existing\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "aug = ImageDataGenerator(rotation_range=20, horizontal_flip=True)\n",
    "\n",
    "# 2. Transfer Learning: Use pre-trained models\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "model = MobileNetV2(weights='imagenet')\n",
    "# Only retrain top layers with your data\n",
    "\n",
    "# 3. Semi-Supervised: Use unlabeled data\n",
    "from sklearn.semi_supervised import LabelSpreading\n",
    "model = LabelSpreading()  # Spreads labels to unlabeled data\n",
    "\n",
    "# 4. Weak Supervision: Use noisy rules\n",
    "def weak_label(text):\n",
    "    if 'great' in text: return 1\n",
    "    elif 'bad' in text: return 0\n",
    "    else: return -1  # Uncertain\n",
    "\n",
    "# 5. Active Learning: Label only most informative samples\n",
    "# Start with small labeled set\n",
    "# Ask human to label 10 most uncertain samples\n",
    "# Retrain, repeat\n",
    "\n",
    "# 6. Crowdsourcing: Many cheap annotators\n",
    "# 100 people label 100 samples each = 10K labels cheap\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Bottom line:** Labeling is bottleneck, use these strategies!\n",
    "\n",
    "---\n",
    "\n",
    "## 3. NON-REPRESENTATIVE DATA\n",
    "\n",
    "**Problem:** Dataset doesn't match real-world population\n",
    "\n",
    "**Two types:**\n",
    "\n",
    "**Sampling Noise (Random):**\n",
    "- Natural variation when sampling\n",
    "- Solution: Increase sample size (reduces by \u221an)\n",
    "\n",
    "**Sampling Bias (Systematic):**\n",
    "- Survey in library only: Misses non-studious students\n",
    "- Man-on-street interview: Only healthy, mobile people\n",
    "- Email dataset from Gmail only: Misses other providers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect bias\n",
    "population_stats = {'gender': {'M': 0.49, 'F': 0.51}}\n",
    "dataset_stats = {'gender': {'M': 0.70, 'F': 0.30}}\n",
    "# Divergence: 0.20 (20%) \u2192 BIASED!\n",
    "\n",
    "# Solutions:\n",
    "# 1. Stratified sampling: Preserve proportions\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "# 2. Reweight samples: Give less weight to overrepresented groups\n",
    "weights = np.array([0.49/0.70 if gender=='M' else 0.51/0.30 for gender in data])\n",
    "model.fit(X, y, sample_weight=weights)\n",
    "\n",
    "# 3. Collect representative data: Ensure all groups included\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Impact:** Unfair models, poor generalization\n",
    "\n",
    "---\n",
    "\n",
    "## 4. POOR DATA QUALITY\n",
    "\n",
    "**Definition:** Garbage IN \u2192 Garbage OUT\n",
    "\n",
    "**MIT: 82% of ML projects stall due to DATA QUALITY issues**\n",
    "\n",
    "**Types:**\n",
    "- Missing values: Incomplete data\n",
    "- Outliers: Extreme values skewing results\n",
    "- Inconsistencies: \"john\" vs \"JOHN\" vs \"john \"\n",
    "- Duplicates: Same record twice\n",
    "- Errors: Wrong data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solutions:\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Missing values\n",
    "data.fillna(data.mean())  # Simple\n",
    "from sklearn.impute import KNNImputer\n",
    "imputer.fit_transform(data)  # Better\n",
    "\n",
    "# Outliers\n",
    "from sklearn.ensemble import IsolationForest\n",
    "iso = IsolationForest()\n",
    "outliers = iso.fit_predict(data)\n",
    "\n",
    "# Inconsistencies\n",
    "data['name'] = data['name'].str.strip().str.lower()\n",
    "data['gender'] = data['gender'].map({'M': 'male', 'F': 'female'})\n",
    "\n",
    "# Duplicates\n",
    "data.drop_duplicates()\n",
    "\n",
    "# Validation\n",
    "def validate_data(data):\n",
    "    print(\"Missing:\", data.isnull().sum())\n",
    "    print(\"Duplicates:\", data.duplicated().sum())\n",
    "    print(\"Summary:\", data.describe())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Impact:** Model accuracy drops 10-50%+\n",
    "\n",
    "---\n",
    "\n",
    "## 5. IRRELEVANT FEATURES\n",
    "\n",
    "**Problem:** Including garbage features hurts performance\n",
    "\n",
    "```\n",
    "More irrelevant features = Worse model!\n",
    "```\n",
    "\n",
    "**Solutions:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Filter Methods (fast, statistical)\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "selector = SelectKBest(f_classif, k=10)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "# 2. Wrapper Methods (uses model)\n",
    "from sklearn.feature_selection import RFE\n",
    "rfe = RFE(estimator, n_features_to_select=10)\n",
    "X_rfe = rfe.fit_transform(X, y)\n",
    "\n",
    "# 3. Embedded Methods (built into model)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rf = RandomForestClassifier()\n",
    "rf.fit(X, y)\n",
    "importances = rf.feature_importances_\n",
    "\n",
    "# 4. Domain Expertise\n",
    "# \"Color doesn't matter for house prices\"\n",
    "# Keep: square_feet, bedrooms, location\n",
    "# Remove: owner_height, owner_age\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Impact:** 10-50% performance improvement possible!\n",
    "\n",
    "---\n",
    "\n",
    "## 6. OVERFITTING\n",
    "\n",
    "**Problem:** Model learns noise, not patterns\n",
    "\n",
    "```\n",
    "Training Accuracy: 99%\n",
    "Test Accuracy: 40%  \u2190 Model overfitted!\n",
    "```\n",
    "\n",
    "**Causes:**\n",
    "- Model too complex\n",
    "- Too little training data\n",
    "- No regularization\n",
    "- Too much training time\n",
    "\n",
    "**Solutions:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Regularization (L1/L2)\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "ridge = Ridge(alpha=1.0)  # Higher alpha = more regularization\n",
    "\n",
    "# 2. Reduce complexity\n",
    "model = RandomForestClassifier(\n",
    "    max_depth=5,           # Shallow trees\n",
    "    min_samples_split=10   # More samples needed to split\n",
    ")\n",
    "\n",
    "# 3. More training data\n",
    "# Get more samples, reduce noise\n",
    "\n",
    "# 4. Early stopping (Neural Networks)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "model.fit(X, y, callbacks=[early_stop])\n",
    "\n",
    "# 5. Dropout (Neural Networks)\n",
    "layers.Dropout(0.3)  # Drop 30% of neurons\n",
    "\n",
    "# 6. Cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "if huge gap between train/test: Overfitting!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Detection:**\n",
    "- Gap between train and test accuracy > 10%\n",
    "- Train accuracy near 100%, test accuracy much lower\n",
    "\n",
    "---\n",
    "\n",
    "## 7. UNDERFITTING\n",
    "\n",
    "**Problem:** Model too simple to learn patterns\n",
    "\n",
    "```\n",
    "Training Accuracy: 60%\n",
    "Test Accuracy: 58%  \u2190 Model underfitted!\n",
    "```\n",
    "\n",
    "**Causes:**\n",
    "- Model too simple\n",
    "- Insufficient training\n",
    "- Poor features\n",
    "- Too much regularization\n",
    "\n",
    "**Solutions:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. More complex model\n",
    "simple = LinearRegression()        # Underfitting risk\n",
    "complex = RandomForestClassifier() # Better\n",
    "\n",
    "# 2. Feature engineering\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "poly = PolynomialFeatures(2)\n",
    "X_poly = poly.fit_transform(X)\n",
    "\n",
    "# 3. Train longer\n",
    "model.fit(X, y, epochs=100)  # More iterations\n",
    "\n",
    "# 4. Reduce regularization\n",
    "ridge = Ridge(alpha=0.01)  # Lower alpha = less regularization\n",
    "\n",
    "# 5. Better features\n",
    "# Domain expertise: \"Add weather data for crop prediction\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Detection:**\n",
    "- Low accuracy on both train and test\n",
    "- No improvement with more data\n",
    "\n",
    "---\n",
    "\n",
    "## 8. SOFTWARE INTEGRATION\n",
    "\n",
    "**Problem:** Model works offline, fails in production\n",
    "\n",
    "**Challenges:**\n",
    "- Environment mismatch (different Python version, packages)\n",
    "- Model serving (how to use in production)\n",
    "- Feature inconsistency (computed differently)\n",
    "- Monitoring (can't debug failures)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solutions:\n",
    "\n",
    "# 1. Containerization (Docker)\n",
    "# Same environment everywhere!\n",
    "dockerfile = \"\"\"\n",
    "FROM python:3.9\n",
    "COPY requirements.txt .\n",
    "RUN pip install -r requirements.txt\n",
    "COPY model.pkl .\n",
    "CMD [\"python\", \"api.py\"]\n",
    "\"\"\"\n",
    "\n",
    "# 2. Flask API\n",
    "from flask import Flask, request\n",
    "app = Flask(__name__)\n",
    "model = pickle.load(open('model.pkl'))\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.json\n",
    "    return {'prediction': float(model.predict([data['features']])[0])}\n",
    "\n",
    "# 3. Feature Store (consistent features)\n",
    "from feast import FeatureStore\n",
    "feature_store = FeatureStore(repo_path='.')\n",
    "# Same features in training and production!\n",
    "\n",
    "# 4. Logging\n",
    "import logging\n",
    "logging.info(f\"Input: {X}\")\n",
    "logging.info(f\"Prediction: {pred}\")\n",
    "logging.error(f\"Error: {e}\")\n",
    "\n",
    "# 5. Monitoring\n",
    "from prometheus_client import Counter\n",
    "prediction_counter = Counter('predictions_total', 'Total')\n",
    "prediction_counter.inc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cost:** $50K - $500K for infrastructure\n",
    "\n",
    "---\n",
    "\n",
    "## 9. OFFLINE LEARNING & CONCEPT DRIFT\n",
    "\n",
    "**Problem:** Model trained on old data performs poorly on new data\n",
    "\n",
    "```\n",
    "Trained on: 2020 data (normal market)\n",
    "Deployed in: 2024 (market changed!)\n",
    "Result: Model predictions way off\n",
    "```\n",
    "\n",
    "**Examples:**\n",
    "- Spam filter trained 2020, new spam types 2024\n",
    "- Stock predictor trained pre-pandemic, deployed 2021 crash\n",
    "- Product recommendation learned old preferences, users changed\n",
    "\n",
    "**Solutions:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Detect drift\n",
    "from scipy.stats import ks_2samp\n",
    "stat, p_value = ks_2samp(X_train, X_recent)\n",
    "if p_value < 0.05:\n",
    "    print(\"\u26a0\ufe0f  Data distribution changed!\")\n",
    "\n",
    "# 2. Online learning (continuous updates)\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "model = SGDClassifier()\n",
    "for batch in new_data:\n",
    "    model.partial_fit(batch)\n",
    "\n",
    "# 3. Scheduled retraining\n",
    "# Retrain model every week with recent data\n",
    "schedule.every().monday.at(\"02:00\").do(retrain)\n",
    "\n",
    "# 4. Ensemble of models\n",
    "# Combine old model + recent model\n",
    "# Recent gets more weight\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cost:** Requires continuous monitoring and retraining\n",
    "\n",
    "---\n",
    "\n",
    "## 10. COST\n",
    "\n",
    "**Problem:** ML projects are expensive!\n",
    "\n",
    "**Breakdown:**\n",
    "- Data labeling: $10K - $1M\n",
    "- Computing (GPUs): $1K - $100K/month\n",
    "- Personnel (DS, Engineer): $150K - $250K/year each\n",
    "- Infrastructure: $5K - $100K\n",
    "- **Total Year 1: $200K - $2M+**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimate_cost(dataset_size, complexity):\n",
    "    annotation_cost = dataset_size * {'simple': 0.10, 'complex': 5.00}[complexity]\n",
    "    personnel = {'simple': 1, 'complex': 4}[complexity] * 75_000\n",
    "    compute = {'simple': 5_000, 'complex': 100_000}[complexity]\n",
    "    total = annotation_cost + personnel + compute\n",
    "    return total\n",
    "\n",
    "# Example: 50K samples, moderate complexity\n",
    "cost = estimate_cost(50_000, 'complex')\n",
    "# \u2192 ~$375K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solutions:**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Transfer Learning\n",
    "# Use pre-trained model: Save $400K!\n",
    "from transformers import AutoModel\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "model.fit(your_data, epochs=5)\n",
    "\n",
    "# 2. Data Augmentation\n",
    "# 10K\u2192100K samples: Save $40K\n",
    "from imgaug import augmenters\n",
    "aug = augmenters.SomeOf([...])\n",
    "augmented = [aug(img) for img in images for _ in range(10)]\n",
    "\n",
    "# 3. AutoML\n",
    "# Replace $150K data scientist with $50/month tool\n",
    "from h2o import automl\n",
    "aml = automl.H2OAutoML(max_models=20)\n",
    "aml.train(X=X, y=y)\n",
    "\n",
    "# 4. Smart prioritization\n",
    "# Only build if ROI > 300%\n",
    "roi = (annual_benefit - annual_cost) / annual_cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**When NOT to build:**\n",
    "- ROI negative\n",
    "- Data doesn't exist\n",
    "- Simple rules work better\n",
    "- Ethical concerns\n",
    "\n",
    "---\n",
    "\n",
    "## DECISION MATRIX: Which Challenge Do You Have?\n",
    "\n",
    "| Symptom | Challenge | Fix |\n",
    "|---------|-----------|-----|\n",
    "| Can't get data | Data Collection | Use APIs, crowdsource, buy |\n",
    "| Data expensive to label | Insufficient Data | Transfer learning, data augmentation |\n",
    "| Model biased, unfair | Non-Representative | Stratified sampling, reweight |\n",
    "| Garbage values, missing | Poor Quality | Cleaning, validation |\n",
    "| Performance unchanged after adding features | Irrelevant Features | Feature selection |\n",
    "| Train: 99%, Test: 40% | Overfitting | Regularization, more data |\n",
    "| Train: 60%, Test: 58% | Underfitting | Complex model, features |\n",
    "| Works offline, fails live | Integration | Docker, monitoring, logging |\n",
    "| Accuracy drops over time | Offline Learning | Online updates, retraining |\n",
    "| Too expensive | Cost | Transfer learning, AutoML |\n",
    "\n",
    "---\n",
    "\n",
    "## QUICK CHECKLIST\n",
    "\n",
    "Before starting ML project:\n",
    "- [ ] Understand ROI (is it worth it?)\n",
    "- [ ] Have/can get data (quality & quantity)\n",
    "- [ ] Budget allocated ($200K minimum)\n",
    "- [ ] Team available (data scientist, engineer)\n",
    "- [ ] Ethical/regulatory OK\n",
    "- [ ] Success metric defined\n",
    "\n",
    "During development:\n",
    "- [ ] Clean data thoroughly\n",
    "- [ ] Check for biases (representative?)\n",
    "- [ ] Monitor train vs test accuracy\n",
    "- [ ] Feature selection done\n",
    "- [ ] Cross-validation used\n",
    "- [ ] Documented everything\n",
    "\n",
    "Before deployment:\n",
    "- [ ] Containerized (Docker)\n",
    "- [ ] Monitoring/logging ready\n",
    "- [ ] Rollback plan\n",
    "- [ ] A/B testing plan\n",
    "- [ ] Retraining schedule\n",
    "\n",
    "After deployment:\n",
    "- [ ] Monitor performance daily\n",
    "- [ ] Detect data drift\n",
    "- [ ] Retrain on schedule\n",
    "- [ ] Log everything\n",
    "- [ ] Handle edge cases\n",
    "\n",
    "---\n",
    "\n",
    "**Print this for your desk!** Reference when facing real ML challenges.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}