{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. POOR QUALITY DATA\n",
    "\n",
    "### Definition\n",
    "**Garbage In, Garbage Out (GIGO)**: Bad data input = Bad predictions output.\n",
    "\n",
    "### Why Data Quality Matters:\n",
    "```\n",
    "MIT Research:\n",
    "- 82% of ML projects STALL due to DATA QUALITY issues\n",
    "- Not because of bad algorithms, but bad data!\n",
    "\n",
    "Alation Report:\n",
    "- 87% of data quality errors impact business outcomes\n",
    "```\n",
    "\n",
    "### Types of Data Quality Issues:\n",
    "\n",
    "#### 4.1 Missing Values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create dataset with missing values\n",
    "data = pd.DataFrame({\n",
    "    'age': [25, None, 35, 40, None, 28],\n",
    "    'income': [50000, 60000, None, 80000, 70000, None],\n",
    "    'credit_score': [720, 750, 680, None, 700, 760]\n",
    "})\n",
    "\n",
    "print(\"Missing values:\")\n",
    "print(data.isnull().sum())\n",
    "\n",
    "# Problems:\n",
    "# - Missing values reduce training data\n",
    "# - Some algorithms can't handle missing values\n",
    "# - Impacts model accuracy\n",
    "\n",
    "# Solutions:\n",
    "\n",
    "# 1. Drop rows with missing values (simple, loses data)\n",
    "data_dropped = data.dropna()\n",
    "print(f\"\\nAfter dropping: {len(data)} \u2192 {len(data_dropped)} rows\")\n",
    "\n",
    "# 2. Fill with mean/median (preserves data, introduces bias)\n",
    "data_filled_mean = data.fillna(data.mean())\n",
    "\n",
    "# 3. Fill with forward/backward fill (for time series)\n",
    "data_filled_forward = data.fillna(method='ffill')\n",
    "\n",
    "# 4. Predictive imputation (uses other features)\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=3)\n",
    "data_imputed = pd.DataFrame(\n",
    "    imputer.fit_transform(data),\n",
    "    columns=data.columns\n",
    ")\n",
    "\n",
    "# 5. Use algorithms that handle missing values\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "model = XGBClassifier()  # Handles missing values natively\n",
    "model.fit(data, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Outliers and Anomalies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Dataset with outliers\n",
    "data = np.random.normal(100, 15, 1000)  # Normal data\n",
    "data = np.append(data, [500, 600, -100])  # Add outliers\n",
    "\n",
    "print(f\"Mean: {np.mean(data):.1f}\")\n",
    "print(f\"Median: {np.median(data):.1f}\")\n",
    "# Outliers skew mean!\n",
    "\n",
    "# Detect outliers\n",
    "# Method 1: Z-score\n",
    "from scipy import stats\n",
    "z_scores = np.abs(stats.zscore(data))\n",
    "outliers_zscore = data[z_scores > 3]  # Beyond 3 std deviations\n",
    "\n",
    "# Method 2: IQR (Interquartile Range)\n",
    "Q1 = np.percentile(data, 25)\n",
    "Q3 = np.percentile(data, 75)\n",
    "IQR = Q3 - Q1\n",
    "outliers_iqr = data[(data < Q1 - 1.5*IQR) | (data > Q3 + 1.5*IQR)]\n",
    "\n",
    "# Method 3: Isolation Forest\n",
    "iso_forest = IsolationForest(contamination=0.05)  # Expect 5% outliers\n",
    "outlier_predictions = iso_forest.fit_predict(data.reshape(-1, 1))\n",
    "\n",
    "# Handle outliers\n",
    "# Option 1: Remove\n",
    "data_no_outliers = data[np.abs(stats.zscore(data)) < 3]\n",
    "\n",
    "# Option 2: Cap at percentiles\n",
    "data_capped = np.clip(data, np.percentile(data, 1), np.percentile(data, 99))\n",
    "\n",
    "# Option 3: Transform (log, sqrt)\n",
    "data_transformed = np.log(data[data > 0])  # Log transformation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Inconsistent Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Inconsistencies\n",
    "data = pd.DataFrame({\n",
    "    'name': ['John', 'JOHN', 'john ', 'Jane', 'jane'],  # Inconsistent case/spacing\n",
    "    'gender': ['M', 'Male', 'male', 'F', 'female'],  # Different formats\n",
    "    'age': [25, 25.0, '25', 26, '26']  # Different types\n",
    "})\n",
    "\n",
    "print(\"Before cleaning:\")\n",
    "print(data)\n",
    "\n",
    "# Clean inconsistencies\n",
    "data['name'] = data['name'].str.strip().str.lower()\n",
    "data['gender'] = data['gender'].map({\n",
    "    'M': 'male',\n",
    "    'F': 'female',\n",
    "    'Male': 'male',\n",
    "    'male': 'male',\n",
    "    'female': 'female',\n",
    "    'F': 'female'\n",
    "})\n",
    "data['age'] = pd.to_numeric(data['age'])\n",
    "\n",
    "print(\"\\nAfter cleaning:\")\n",
    "print(data)\n",
    "\n",
    "# Use mapping dictionaries\n",
    "gender_mapping = {\n",
    "    'M': 'male', 'm': 'male', 'Male': 'male',\n",
    "    'F': 'female', 'f': 'female', 'Female': 'female'\n",
    "}\n",
    "data['gender'] = data['gender'].map(gender_mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.4 Duplicate Records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Dataset with duplicates\n",
    "data = pd.DataFrame({\n",
    "    'customer_id': [1, 2, 3, 2, 1, 4],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Bob', 'Alice', 'David']\n",
    "})\n",
    "\n",
    "print(f\"Original size: {len(data)}\")\n",
    "print(\"\\nDuplicate rows:\")\n",
    "print(data[data.duplicated()])\n",
    "\n",
    "# Remove duplicates\n",
    "data_unique = data.drop_duplicates()\n",
    "print(f\"\\nAfter removing duplicates: {len(data_unique)}\")\n",
    "\n",
    "# Keep first, last, or most recent occurrence\n",
    "data_first = data.drop_duplicates(subset=['customer_id'], keep='first')\n",
    "data_last = data.drop_duplicates(subset=['customer_id'], keep='last')\n",
    "\n",
    "# Aggregate duplicates (instead of removing)\n",
    "data_aggregated = data.groupby('customer_id').agg({\n",
    "    'name': 'first'\n",
    "})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5 Data Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_data(data):\n",
    "    \"\"\"Check data quality\"\"\"\n",
    "    \n",
    "    print(\"=== DATA QUALITY REPORT ===\\n\")\n",
    "    \n",
    "    # Check 1: Missing values\n",
    "    missing = data.isnull().sum()\n",
    "    if missing.any():\n",
    "        print(\"\u274c Missing values:\")\n",
    "        print(missing[missing > 0])\n",
    "        print()\n",
    "    \n",
    "    # Check 2: Duplicates\n",
    "    duplicates = data.duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"\u274c Duplicate rows: {duplicates}\")\n",
    "        print()\n",
    "    \n",
    "    # Check 3: Data types\n",
    "    print(\"Data types:\")\n",
    "    print(data.dtypes)\n",
    "    print()\n",
    "    \n",
    "    # Check 4: Outliers\n",
    "    print(\"Outlier detection (Z-score > 3):\")\n",
    "    from scipy import stats\n",
    "    for col in data.select_dtypes(include=[np.number]).columns:\n",
    "        outliers = (np.abs(stats.zscore(data[col].dropna())) > 3).sum()\n",
    "        if outliers > 0:\n",
    "            print(f\"  {col}: {outliers} outliers\")\n",
    "    \n",
    "    # Check 5: Statistical summary\n",
    "    print(\"\\nStatistical summary:\")\n",
    "    print(data.describe())\n",
    "\n",
    "# Usage\n",
    "validate_data(dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impact of Poor Data Quality:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# True relationship: y = 2*x + noise\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y_clean = 2*X.ravel() + np.random.normal(0, 1, 100)\n",
    "\n",
    "# Introduce data quality issues\n",
    "y_dirty = y_clean.copy()\n",
    "# Add outliers\n",
    "y_dirty[0] = 1000\n",
    "y_dirty[50] = -1000\n",
    "# Add missing values (just use wrong values)\n",
    "y_dirty[10:15] = np.nan\n",
    "\n",
    "# Remove NaNs for comparison\n",
    "mask = ~np.isnan(y_dirty)\n",
    "\n",
    "# Train on clean data\n",
    "model_clean = LinearRegression()\n",
    "model_clean.fit(X, y_clean)\n",
    "pred_clean = model_clean.predict(X)\n",
    "mse_clean = mean_squared_error(y_clean, pred_clean)\n",
    "\n",
    "# Train on dirty data\n",
    "model_dirty = LinearRegression()\n",
    "model_dirty.fit(X[mask], y_dirty[mask])\n",
    "pred_dirty = model_dirty.predict(X[mask])\n",
    "mse_dirty = mean_squared_error(y_clean[mask], pred_dirty[mask])\n",
    "\n",
    "print(f\"Clean data MSE: {mse_clean:.2f}\")\n",
    "print(f\"Dirty data MSE: {mse_dirty:.2f}\")\n",
    "print(f\"Performance degradation: {(mse_dirty/mse_clean - 1)*100:.0f}%\")\n",
    "# Dirty data causes massive performance loss!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}