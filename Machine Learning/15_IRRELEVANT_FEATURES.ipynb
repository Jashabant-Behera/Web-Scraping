{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. IRRELEVANT FEATURES\n",
    "\n",
    "### Definition\n",
    "**Garbage IN \u2192 Garbage OUT**: Including irrelevant features hurts model performance.\n",
    "\n",
    "### Why Irrelevant Features Are Bad:\n",
    "\n",
    "```\n",
    "1. Noise introduction: Model confuses signal with noise\n",
    "2. Overfitting risk: Model learns random patterns\n",
    "3. Computational cost: Training slower, higher memory\n",
    "4. Interpretability: Hard to explain predictions\n",
    "5. Curse of dimensionality: Performance degrades in high dimensions\n",
    "```\n",
    "\n",
    "### Example: Irrelevant Feature Problem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create synthetic data\n",
    "X, y = make_regression(n_samples=200, n_features=10, n_informative=5, noise=10)\n",
    "\n",
    "# Add completely irrelevant (random noise) features\n",
    "n_irrelevant = [0, 5, 10, 20, 50]\n",
    "scores = []\n",
    "\n",
    "for n_irrel in n_irrelevant:\n",
    "    # Add random noise features\n",
    "    X_with_noise = np.column_stack([\n",
    "        X,\n",
    "        np.random.randn(200, n_irrel)\n",
    "    ])\n",
    "    \n",
    "    # Train model\n",
    "    model = RandomForestRegressor(n_estimators=50)\n",
    "    score = cross_val_score(model, X_with_noise, y, cv=5).mean()\n",
    "    scores.append(score)\n",
    "    \n",
    "    print(f\"Features: {X_with_noise.shape[1]}, Score: {score:.3f}\")\n",
    "\n",
    "# Plot: More irrelevant features = worse performance!\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(n_irrelevant, scores, marker='o', linewidth=2)\n",
    "plt.xlabel('Number of Irrelevant Features')\n",
    "plt.ylabel('Model R\u00b2 Score')\n",
    "plt.title('Effect of Irrelevant Features on Model Performance')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Selection Methods:\n",
    "\n",
    "#### 1. **Filter Methods** (Fast, independent of model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "# Load data\n",
    "cancer = load_breast_cancer()\n",
    "X = pd.DataFrame(cancer.data, columns=cancer.feature_names)\n",
    "y = cancer.target\n",
    "\n",
    "print(f\"Original features: {X.shape[1]}\")\n",
    "\n",
    "# Method 1: Statistical tests (ANOVA F-score)\n",
    "selector = SelectKBest(score_func=f_classif, k=10)\n",
    "X_selected = selector.fit_transform(X, y)\n",
    "\n",
    "print(f\"After SelectKBest: {X_selected.shape[1]}\")\n",
    "\n",
    "# See which features were selected\n",
    "selected_features = X.columns[selector.get_support()]\n",
    "print(\"\\nSelected features:\")\n",
    "print(selected_features.tolist())\n",
    "\n",
    "# Method 2: Mutual information\n",
    "selector_mi = SelectKBest(score_func=mutual_info_classif, k=10)\n",
    "X_selected_mi = selector_mi.fit_transform(X, y)\n",
    "\n",
    "# Method 3: Correlation with target\n",
    "correlations = X.corrwith(y).abs().sort_values(ascending=False)\n",
    "print(\"\\nTop 10 correlated features:\")\n",
    "print(correlations.head(10))\n",
    "\n",
    "top_features = correlations.head(10).index\n",
    "X_filtered = X[top_features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. **Wrapper Methods** (Uses model performance to select)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Recursive Feature Elimination\n",
    "model = RandomForestClassifier(n_estimators=50)\n",
    "rfe = RFE(model, n_features_to_select=10)\n",
    "X_rfe = rfe.fit_transform(X, y)\n",
    "\n",
    "print(\"RFE Selected features:\")\n",
    "selected = X.columns[rfe.support_]\n",
    "print(selected.tolist())\n",
    "\n",
    "# Forward/Backward selection\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "\n",
    "sfs = SFS(\n",
    "    model,\n",
    "    k_features=10,\n",
    "    forward=True,  # Forward (start with empty), False for backward\n",
    "    verbose=1,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "sfs.fit(X, y)\n",
    "print(\"\\nSequential selection features:\")\n",
    "print(list(sfs.k_feature_names_))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. **Embedded Methods** (Features selected during training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Tree-based feature importance\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Get feature importances\n",
    "importances = pd.DataFrame({\n",
    "    'feature': cancer.feature_names,\n",
    "    'importance': model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"Top 10 important features:\")\n",
    "print(importances.head(10))\n",
    "\n",
    "# Select features above threshold\n",
    "selector = SelectFromModel(model, prefit=True, threshold='median')\n",
    "X_selected = selector.transform(X)\n",
    "\n",
    "print(f\"Features selected: {X_selected.shape[1]}/{X.shape[1]}\")\n",
    "\n",
    "# L1 regularization (Lasso)\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "l1_model = LogisticRegression(penalty='l1', solver='liblinear', C=1.0)\n",
    "l1_model.fit(X, y)\n",
    "\n",
    "# Non-zero coefficients = selected features\n",
    "selected_l1 = X.columns[l1_model.coef_[0] != 0]\n",
    "print(f\"\\nL1 selected: {len(selected_l1)} features\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. **Domain Expertise**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sometimes simple domain knowledge beats automated selection!\n",
    "\n",
    "# Example: Predicting house prices\n",
    "all_features = [\n",
    "    'square_feet',      # \u2705 Obviously important\n",
    "    'bedrooms',         # \u2705 Obviously important\n",
    "    'bathrooms',        # \u2705 Obviously important\n",
    "    'location',         # \u2705 Obviously important\n",
    "    'color',            # \u274c Irrelevant\n",
    "    'owner_height',     # \u274c Irrelevant\n",
    "    'owner_age',        # \u274c Irrelevant (generally)\n",
    "    'year_built',       # \u2705 Important\n",
    "]\n",
    "\n",
    "# Use domain expertise to select\n",
    "important_features = [\n",
    "    'square_feet',\n",
    "    'bedrooms',\n",
    "    'bathrooms',\n",
    "    'location',\n",
    "    'year_built'\n",
    "]\n",
    "\n",
    "X_selected = X[important_features]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}