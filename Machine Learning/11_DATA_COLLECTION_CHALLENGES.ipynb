{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DATA COLLECTION CHALLENGES\n",
    "\n",
    "### Definition\n",
    "**Data Collection** is the foundation of ML projects. Poor collection = poor models, no matter how sophisticated algorithms are.\n",
    "\n",
    "### Challenge Overview\n",
    "Getting right data is 80% of ML work, yet often overlooked in favor of model building.\n",
    "\n",
    "```\n",
    "Good Model + Bad Data = Bad Predictions \u274c\n",
    "Bad Model + Good Data = Can be improved \u2705\n",
    "```\n",
    "\n",
    "### 1.1 Web Scraping for Data Collection\n",
    "\n",
    "#### What is Web Scraping?\n",
    "Extracting data from websites programmatically instead of manually copying.\n",
    "\n",
    "#### Challenges with Web Scraping:\n",
    "\n",
    "**1. Legal & Ethical Issues**\n",
    "- Many websites forbid scraping in Terms of Service\n",
    "- Copyright concerns for extracted content\n",
    "- GDPR, CCPA compliance required\n",
    "- Legal action possible from website owners\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: What NOT to do (scraping without permission)\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# \u274c Scraping without checking robots.txt\n",
    "url = \"https://example.com/data\"\n",
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "data = soup.find_all('div', class_='data')\n",
    "# Might violate ToS!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Anti-Scraping Protection**\n",
    "- Websites block scrapers with CAPTCHA\n",
    "- IP blocking after multiple requests\n",
    "- JavaScript rendering required (static HTML won't work)\n",
    "- Rate limiting and throttling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Handling anti-scraping measures\n",
    "import time\n",
    "from selenium import webdriver\n",
    "\n",
    "# \u274c Simple approach: Gets blocked\n",
    "for i in range(1000):\n",
    "    page = requests.get(f'https://example.com/page/{i}')\n",
    "    # IP gets blocked after ~10 requests\n",
    "\n",
    "# \u2705 Better approach: Use proxy rotation + delays\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.common.by import By\n",
    "\n",
    "driver = Chrome()\n",
    "\n",
    "for i in range(100):\n",
    "    driver.get(f'https://example.com/page/{i}')\n",
    "    time.sleep(2)  # Respectful delay\n",
    "    data = driver.find_elements(By.CLASS_NAME, 'data')\n",
    "    # Process data\n",
    "    \n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Dynamic Content**\n",
    "- Websites load data with JavaScript\n",
    "- Content not in initial HTML\n",
    "- Requires browser automation (slow, resource-intensive)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Dynamic content challenges\n",
    "# \u274c Won't work: BeautifulSoup only gets initial HTML\n",
    "response = requests.get('https://spa-website.com')\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "data = soup.find_all('div', class_='dynamic-content')\n",
    "# Empty! Content loaded by JavaScript\n",
    "\n",
    "# \u2705 Solution: Use Selenium\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://spa-website.com')\n",
    "\n",
    "# Wait for JavaScript to render\n",
    "element = WebDriverWait(driver, 10).until(\n",
    "    EC.presence_of_all_elements_located((By.CLASS_NAME, \"dynamic-content\"))\n",
    ")\n",
    "\n",
    "data = driver.find_elements(By.CLASS_NAME, 'dynamic-content')\n",
    "print(f\"Found {len(data)} items\")\n",
    "driver.quit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Best Practices for Web Scraping:\n",
    "1. **Check robots.txt:** `https://example.com/robots.txt`\n",
    "2. **Respect ToS:** Read Terms of Service\n",
    "3. **Add delays:** Use `time.sleep()` between requests\n",
    "4. **Identify yourself:** Set proper User-Agent headers\n",
    "5. **Use APIs first:** If available (easier, legal, faster)\n",
    "6. **Rotate IPs:** Use proxy services for large-scale scraping\n",
    "7. **Monitor rate:** Don't overload servers\n",
    "\n",
    "### 1.2 API-Based Data Collection\n",
    "\n",
    "#### What are APIs?\n",
    "Structured way to request data from services (better than scraping).\n",
    "\n",
    "#### Advantages:\n",
    "- \u2705 Legal and authorized\n",
    "- \u2705 Structured data format (JSON, XML)\n",
    "- \u2705 Real-time updates\n",
    "- \u2705 Rate limiting is fair\n",
    "- \u2705 Documentation available\n",
    "\n",
    "#### Challenges:\n",
    "\n",
    "**1. Rate Limiting**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Hitting rate limits\n",
    "import requests\n",
    "import time\n",
    "\n",
    "API_URL = \"https://api.example.com/data\"\n",
    "API_KEY = \"your-api-key\"\n",
    "\n",
    "# \u274c Too fast: Rate limit exceeded\n",
    "for i in range(1000):\n",
    "    response = requests.get(\n",
    "        API_URL,\n",
    "        params={'id': i},\n",
    "        headers={'Authorization': f'Bearer {API_KEY}'}\n",
    "    )\n",
    "    # Error: 429 Too Many Requests\n",
    "\n",
    "# \u2705 Respectful approach: Use delays and backoff\n",
    "import random\n",
    "\n",
    "for i in range(1000):\n",
    "    try:\n",
    "        response = requests.get(\n",
    "            API_URL,\n",
    "            params={'id': i},\n",
    "            headers={'Authorization': f'Bearer {API_KEY}'},\n",
    "            timeout=10\n",
    "        )\n",
    "        \n",
    "        if response.status_code == 429:  # Rate limited\n",
    "            wait_time = int(response.headers.get('Retry-After', 60))\n",
    "            print(f\"Rate limited. Waiting {wait_time}s...\")\n",
    "            time.sleep(wait_time)\n",
    "        elif response.status_code == 200:\n",
    "            data = response.json()\n",
    "            # Process data\n",
    "            time.sleep(random.uniform(1, 3))  # Respectful delay\n",
    "        else:\n",
    "            print(f\"Error: {response.status_code}\")\n",
    "            \n",
    "    except requests.exceptions.Timeout:\n",
    "        print(\"Request timeout, retrying...\")\n",
    "        time.sleep(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Quota Limits**\n",
    "- API calls limited per month/day\n",
    "- Paid plans for more requests\n",
    "- Sometimes insufficient for training data needs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Tracking API quota\n",
    "class APIClient:\n",
    "    def __init__(self, api_key, monthly_quota=1000):\n",
    "        self.api_key = api_key\n",
    "        self.monthly_quota = monthly_quota\n",
    "        self.calls_used = 0\n",
    "    \n",
    "    def get_data(self, endpoint):\n",
    "        if self.calls_used >= self.monthly_quota:\n",
    "            print(f\"\u274c Quota exceeded! {self.calls_used}/{self.monthly_quota}\")\n",
    "            return None\n",
    "        \n",
    "        response = requests.get(\n",
    "            endpoint,\n",
    "            headers={'Authorization': f'Bearer {self.api_key}'}\n",
    "        )\n",
    "        \n",
    "        self.calls_used += 1\n",
    "        return response.json()\n",
    "\n",
    "client = APIClient(monthly_quota=5000)\n",
    "\n",
    "for i in range(10000):\n",
    "    data = client.get_data(f'https://api.example.com/item/{i}')\n",
    "    if data is None:\n",
    "        print(\"Can't collect more data - quota exceeded!\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. Unstable APIs**\n",
    "- Endpoints change or disappear\n",
    "- Response format changes\n",
    "- Service downtime\n",
    "- Authentication issues\n",
    "\n",
    "#### Tools for Data Collection:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1: Beautiful Soup (simple HTML parsing)\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def scrape_simple(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    return soup.find_all('div', class_='item')\n",
    "\n",
    "# Option 2: Selenium (JavaScript rendering)\n",
    "from selenium import webdriver\n",
    "\n",
    "def scrape_dynamic(url):\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "    driver.implicitly_wait(10)\n",
    "    elements = driver.find_elements(\"class name\", \"item\")\n",
    "    driver.quit()\n",
    "    return elements\n",
    "\n",
    "# Option 3: Scrapy (industrial-grade scraping)\n",
    "import scrapy\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "\n",
    "class DataSpider(scrapy.Spider):\n",
    "    name = \"data_spider\"\n",
    "    start_urls = ['https://example.com']\n",
    "    \n",
    "    def parse(self, response):\n",
    "        for item in response.css('div.item'):\n",
    "            yield {\n",
    "                'title': item.css('h2::text').get(),\n",
    "                'price': item.css('span.price::text').get(),\n",
    "            }\n",
    "\n",
    "# Option 4: Requests + Pandas (for APIs)\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def fetch_from_api(api_url):\n",
    "    response = requests.get(api_url)\n",
    "    data = response.json()\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# Option 5: Official Data Packages\n",
    "import kaggle\n",
    "import requests\n",
    "\n",
    "# Download from Kaggle\n",
    "kaggle.api.dataset_download_files('dataset-name')\n",
    "\n",
    "# Use public datasets: OpenML, UCI, Google Datasets, GitHub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real-World Data Collection Scenarios:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scenario 1: E-commerce Product Data\n",
    "# Challenge: Amazon heavily protects against scraping\n",
    "# Solution: Use official APIs or buy pre-scraped datasets\n",
    "import requests\n",
    "\n",
    "def get_ecommerce_data_ethical():\n",
    "    # Use official API instead of scraping\n",
    "    api_key = \"your-api-key\"\n",
    "    response = requests.get(\n",
    "        \"https://api.example.com/products\",\n",
    "        headers={'API-Key': api_key}\n",
    "    )\n",
    "    return response.json()\n",
    "\n",
    "# Scenario 2: Social Media Data\n",
    "# Challenge: APIs have strict rate limits\n",
    "# Solution: Be selective about what/when to collect\n",
    "import tweepy\n",
    "import time\n",
    "\n",
    "def collect_tweets_responsibly():\n",
    "    client = tweepy.Client(bearer_token=\"YOUR_BEARER_TOKEN\")\n",
    "    \n",
    "    tweets = []\n",
    "    for query in [\"python\", \"machine learning\", \"data science\"]:\n",
    "        response = client.search_recent_tweets(\n",
    "            query=query,\n",
    "            max_results=100  # Respect limits\n",
    "        )\n",
    "        tweets.extend(response.data)\n",
    "        time.sleep(2)  # Respectful delay\n",
    "    \n",
    "    return tweets\n",
    "\n",
    "# Scenario 3: Scientific Data\n",
    "# Challenge: Limited availability, high cost\n",
    "# Solution: Use open-source datasets, replicate studies\n",
    "from sklearn.datasets import load_breast_cancer, fetch_20newsgroups\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "# Use established datasets\n",
    "data = load_breast_cancer()\n",
    "# or\n",
    "dataset = tfds.load('mnist')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}