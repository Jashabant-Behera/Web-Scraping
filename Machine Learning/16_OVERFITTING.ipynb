{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. OVERFITTING\n",
    "\n",
    "### Definition\n",
    "**Model learns noise and quirks of training data**, not true patterns. Performs great on training, terrible on test data.\n",
    "\n",
    "```\n",
    "Overfitting = Model memorizes training data\n",
    "              instead of learning generalizable patterns\n",
    "```\n",
    "\n",
    "### Visual Explanation:\n",
    "\n",
    "```\n",
    "Training Data:  \u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\u25cf\n",
    "Underlying Pattern: Straight line (simple)\n",
    "\n",
    "Underfitted Model:    ___________   (too simple, misses pattern)\n",
    "Perfectly Fit Model:  ___________   (captures pattern)\n",
    "Overfitted Model:     ~\u2227~\u2227~\u2227~\u2227~    (wiggly, memorizes noise)\n",
    "```\n",
    "\n",
    "### Code Example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate simple data: y = x + noise\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y_true = X.ravel()\n",
    "y = y_true + np.random.normal(0, 2, 100)  # Add noise\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "# Test different polynomial degrees\n",
    "degrees = [1, 3, 5, 10, 20]\n",
    "train_scores = []\n",
    "test_scores = []\n",
    "\n",
    "for degree in degrees:\n",
    "    # Create polynomial features\n",
    "    poly = PolynomialFeatures(degree)\n",
    "    X_train_poly = poly.fit_transform(X_train)\n",
    "    X_test_poly = poly.transform(X_test)\n",
    "    \n",
    "    # Train model\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train_poly, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_pred = model.predict(X_train_poly)\n",
    "    test_pred = model.predict(X_test_poly)\n",
    "    \n",
    "    train_mse = mean_squared_error(y_train, train_pred)\n",
    "    test_mse = mean_squared_error(y_test, test_pred)\n",
    "    \n",
    "    train_scores.append(train_mse)\n",
    "    test_scores.append(test_mse)\n",
    "    \n",
    "    print(f\"Degree {degree:2d}: Train MSE: {train_mse:.2f}, Test MSE: {test_mse:.2f}\")\n",
    "\n",
    "# Plot: Shows overfitting\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(degrees, train_scores, marker='o', label='Training Error', linewidth=2)\n",
    "plt.plot(degrees, test_scores, marker='s', label='Test Error', linewidth=2)\n",
    "plt.xlabel('Model Complexity (Polynomial Degree)')\n",
    "plt.ylabel('Mean Squared Error')\n",
    "plt.title('Overfitting: Gap between training and test error')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "# Output:\n",
    "# Degree  1: Train MSE: 4.45, Test MSE: 3.92  \u2190 Good generalization\n",
    "# Degree  3: Train MSE: 3.20, Test MSE: 3.45  \u2190 Still good\n",
    "# Degree  5: Train MSE: 2.10, Test MSE: 5.23  \u2190 Starting to overfit\n",
    "# Degree 10: Train MSE: 0.15, Test MSE: 24.50 \u2190 Severe overfitting!\n",
    "# Degree 20: Train MSE: 0.02, Test MSE: 89.20 \u2190 Extreme overfitting!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causes of Overfitting:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Model too complex relative to data\n",
    "#    Solution: Simplify model, reduce parameters\n",
    "\n",
    "# 2. Too much training time\n",
    "#    Solution: Use early stopping\n",
    "\n",
    "# 3. No regularization\n",
    "#    Solution: Add L1/L2 regularization\n",
    "\n",
    "# 4. Too little data\n",
    "#    Solution: Collect more data, data augmentation\n",
    "\n",
    "# 5. Noisy training data\n",
    "#    Solution: Clean data, remove outliers\n",
    "\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Solution 1: Regularization (Lasso/Ridge)\n",
    "ridge = Ridge(alpha=1.0)  # Higher alpha = more regularization\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Solution 2: Reduce model complexity\n",
    "simple_model = RandomForestClassifier(\n",
    "    n_estimators=10,      # Fewer trees\n",
    "    max_depth=5,          # Shallower trees\n",
    "    min_samples_split=10  # More samples required to split\n",
    ")\n",
    "\n",
    "# Solution 3: Cross-validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scores = cross_val_score(model, X, y, cv=5)\n",
    "print(f\"CV scores: {scores}\")  # Detect overfitting early\n",
    "\n",
    "# Solution 4: Early stopping (Neural Networks)\n",
    "early_stop = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,  # Stop if no improvement for 5 epochs\n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_split=0.2,\n",
    "    callbacks=[early_stop],\n",
    "    epochs=100\n",
    ")\n",
    "\n",
    "# Solution 5: Dropout (Neural Networks)\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = Sequential([\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.3),  # Drop 30% of neurons randomly\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.Dropout(0.2),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}