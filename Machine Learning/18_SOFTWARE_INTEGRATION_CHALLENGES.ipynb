{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. SOFTWARE INTEGRATION CHALLENGES\n",
    "\n",
    "### Definition\n",
    "**Deploying ML models to production** and integrating with existing systems is complex and error-prone.\n",
    "\n",
    "### Challenge 1: Version Control & Reproducibility\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem: Can't reproduce model results\n",
    "# \u274c Reproducible = Different results each run!\n",
    "\n",
    "np.random.seed()  # No seed set\n",
    "model = RandomForestClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Different results every time!\n",
    "pred1 = model.predict(X_test)\n",
    "pred2 = model.predict(X_test)  # Different!\n",
    "\n",
    "# \u2705 Solution: Set seed for reproducibility\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Same results every time\n",
    "pred1 = model.predict(X_test)\n",
    "pred2 = model.predict(X_test)  # Identical!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 2: Environment Parity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem: Model works locally but fails in production\n",
    "# Causes:\n",
    "# - Different Python version\n",
    "# - Different package versions\n",
    "# - Different OS\n",
    "# - Different architecture (CPU vs GPU)\n",
    "\n",
    "# Local: Python 3.9, NumPy 1.21\n",
    "# Production: Python 3.10, NumPy 1.24\n",
    "# Result: Model crashes or gives wrong predictions!\n",
    "\n",
    "# \u2705 Solution: Use containers (Docker)\n",
    "import subprocess\n",
    "\n",
    "dockerfile = \"\"\"\n",
    "FROM python:3.9-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY requirements.txt .\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "COPY model.pkl .\n",
    "COPY api.py .\n",
    "\n",
    "CMD [\"python\", \"api.py\"]\n",
    "\"\"\"\n",
    "\n",
    "requirements_txt = \"\"\"\n",
    "numpy==1.21.0\n",
    "scikit-learn==0.24.2\n",
    "flask==2.0.1\n",
    "\"\"\"\n",
    "\n",
    "# Now everyone has identical environment!\n",
    "\n",
    "# \u2705 Solution: Document environment\n",
    "import pkg_resources\n",
    "\n",
    "def save_environment():\n",
    "    \"\"\"Save package versions\"\"\"\n",
    "    installed_packages = pkg_resources.working_set\n",
    "    with open('requirements.txt', 'w') as f:\n",
    "        for package in sorted(installed_packages, key=lambda x: x.key):\n",
    "            f.write(f\"{package.key}=={package.version}\\n\")\n",
    "\n",
    "save_environment()\n",
    "\n",
    "# Others can install same versions:\n",
    "# pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 3: Model Serving Infrastructure\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem: Model needs to handle real-time requests\n",
    "# Solutions:\n",
    "\n",
    "# Option 1: Flask API (simple, single-threaded)\n",
    "from flask import Flask, request, jsonify\n",
    "import pickle\n",
    "\n",
    "app = Flask(__name__)\n",
    "model = pickle.load(open('model.pkl', 'rb'))\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.json\n",
    "    X = data['features']\n",
    "    prediction = model.predict([X])[0]\n",
    "    return jsonify({'prediction': float(prediction)})\n",
    "\n",
    "# Option 2: TensorFlow Serving (production-grade)\n",
    "# Handles multiple versions, A/B testing, etc.\n",
    "\n",
    "# Option 3: Ray Serve (distributed serving)\n",
    "from ray import serve\n",
    "import ray\n",
    "\n",
    "serve.start()\n",
    "\n",
    "@serve.deployment\n",
    "class Model:\n",
    "    def __init__(self):\n",
    "        self.model = pickle.load(open('model.pkl', 'rb'))\n",
    "    \n",
    "    def __call__(self, request):\n",
    "        features = request['features']\n",
    "        return self.model.predict([features])\n",
    "\n",
    "serve.run(Model.bind())\n",
    "\n",
    "# Option 4: KServe (Kubernetes-native)\n",
    "# Orchestrates model serving on K8s clusters\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 4: Data Pipeline Issues\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem: Features computed differently in training vs production\n",
    "# Training: Features computed offline, batch\n",
    "# Production: Features computed in real-time, online\n",
    "# Result: Feature mismatch = poor predictions!\n",
    "\n",
    "# Example: Compute user average spending\n",
    "# Training:\n",
    "average_spending = df.groupby('user_id')['amount'].mean()\n",
    "# Result: {user_1: 100, user_2: 150, ...}\n",
    "\n",
    "# Production:\n",
    "# API computes average from last 30 days\n",
    "average_spending_prod = last_30_days.groupby('user_id')['amount'].mean()\n",
    "# Result: {user_1: 120, user_2: 130, ...}  \u2190 DIFFERENT!\n",
    "\n",
    "# \u2705 Solution: Centralized feature store\n",
    "from feast import FeatureStore\n",
    "\n",
    "# Define features once\n",
    "feature_store = FeatureStore(repo_path='.')\n",
    "\n",
    "# Use in both training and production\n",
    "features_training = feature_store.get_historical_features(...)\n",
    "features_production = feature_store.get_online_features(...)\n",
    "# Now guaranteed to be identical!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Challenge 5: Monitoring & Debugging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Problem: Can't debug what's wrong when model fails in production\n",
    "\n",
    "# \u2705 Solution: Comprehensive logging\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def predict_with_logging(X):\n",
    "    logger.info(f\"Input shape: {X.shape}\")\n",
    "    logger.info(f\"Input values: mean={X.mean():.2f}, std={X.std():.2f}\")\n",
    "    \n",
    "    try:\n",
    "        prediction = model.predict(X)\n",
    "        logger.info(f\"Prediction: {prediction}\")\n",
    "        return prediction\n",
    "    \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Prediction failed: {str(e)}\")\n",
    "        logger.error(f\"Input data: {X}\")\n",
    "        raise\n",
    "\n",
    "# \u2705 Solution: Performance monitoring\n",
    "from prometheus_client import Counter, Histogram\n",
    "import time\n",
    "\n",
    "prediction_counter = Counter('predictions_total', 'Total predictions')\n",
    "prediction_latency = Histogram('prediction_latency_seconds', 'Prediction latency')\n",
    "\n",
    "def predict_monitored(X):\n",
    "    start = time.time()\n",
    "    \n",
    "    prediction = model.predict(X)\n",
    "    \n",
    "    prediction_counter.inc()\n",
    "    prediction_latency.observe(time.time() - start)\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "# \u2705 Solution: Alerting\n",
    "def alert_if_needed(metric_value, threshold):\n",
    "    if metric_value > threshold:\n",
    "        send_alert(f\"\u26a0\ufe0f  Alert: {metric_value} exceeds {threshold}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deployment Checklist:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment_checklist = {\n",
    "    'Code': [\n",
    "        'Code reviewed and tested',\n",
    "        'All edge cases handled',\n",
    "        'Error handling in place',\n",
    "        'Logging comprehensive'\n",
    "    ],\n",
    "    'Model': [\n",
    "        'Model tested offline',\n",
    "        'Cross-validation done',\n",
    "        'Performance meets requirements',\n",
    "        'Model serialized and versioned'\n",
    "    ],\n",
    "    'Infrastructure': [\n",
    "        'Containerized (Docker)',\n",
    "        'Environment documented',\n",
    "        'CI/CD pipeline set up',\n",
    "        'Rollback plan ready'\n",
    "    ],\n",
    "    'Monitoring': [\n",
    "        'Logging configured',\n",
    "        'Metrics tracked',\n",
    "        'Alerts set up',\n",
    "        'Dashboard created'\n",
    "    ],\n",
    "    'Data': [\n",
    "        'Data validation in place',\n",
    "        'Feature store available',\n",
    "        'Data quality monitored',\n",
    "        'Backup and recovery plan'\n",
    "    ]\n",
    "}\n",
    "\n",
    "for category, items in deployment_checklist.items():\n",
    "    print(f\"\\n{category}:\")\n",
    "    for item in items:\n",
    "        print(f\"  \u2610 {item}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.x"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}