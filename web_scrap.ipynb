{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "98b198b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Headers configured and data storage ready!\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "from typing import List, Dict\n",
    "\n",
    "# Headers to avoid bot detection\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'\n",
    "}\n",
    "\n",
    "# Lists to store our data\n",
    "all_companies_data = []\n",
    "\n",
    "print(\"✓ Headers configured and data storage ready!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e90d9118",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1...\n",
      "✓ Successfully fetched page 1\n",
      "HTML content length: 362135 characters\n"
     ]
    }
   ],
   "source": [
    "# This gets the HTML content from the first page\n",
    "page_number = 1\n",
    "url = f'https://www.ambitionbox.com/list-of-companies?page={page_number}'\n",
    "\n",
    "print(f\"Fetching page {page_number}...\")\n",
    "response = requests.get(url, headers=headers)\n",
    "\n",
    "if response.status_code == 200:\n",
    "    html_content = response.text\n",
    "    print(f\"✓ Successfully fetched page {page_number}\")\n",
    "    print(f\"HTML content length: {len(html_content)} characters\")\n",
    "else:\n",
    "    print(f\"✗ Failed to fetch page. Status: {response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7fe376c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20 company names\n",
      "Found 20 ratings\n",
      "\n",
      "✓ Extracted 20 companies from page 1\n",
      "\n",
      "First 5 companies:\n",
      "  1. TCS - Rating: 3.4\n",
      "  2. Accenture - Rating: 3.7\n",
      "  3. Wipro - Rating: 3.6\n",
      "  4. Cognizant - Rating: 3.7\n",
      "  5. Capgemini - Rating: 3.7\n"
     ]
    }
   ],
   "source": [
    "# Parse the HTML and find all company information\n",
    "soup = BeautifulSoup(html_content, 'lxml')\n",
    "\n",
    "# Find company names\n",
    "company_cards = soup.find_all('h2', class_='companyCardWrapper__companyName')\n",
    "print(f\"Found {len(company_cards)} company names\")\n",
    "\n",
    "# Find ratings\n",
    "ratings = soup.find_all('div', class_='rating_text')\n",
    "print(f\"Found {len(ratings)} ratings\")\n",
    "\n",
    "# Store basic company info\n",
    "companies_page1 = []\n",
    "for idx, card in enumerate(company_cards):\n",
    "    company_name = card.get_text(strip=True)\n",
    "    company_rating = ratings[idx].text.strip() if idx < len(ratings) else 'N/A'\n",
    "    \n",
    "    companies_page1.append({\n",
    "        'company_name': company_name,\n",
    "        'rating': company_rating\n",
    "    })\n",
    "\n",
    "print(f\"\\n✓ Extracted {len(companies_page1)} companies from page 1\")\n",
    "print(\"\\nFirst 5 companies:\")\n",
    "for i, comp in enumerate(companies_page1[:5], 1):\n",
    "    print(f\"  {i}. {comp['company_name']} - Rating: {comp['rating']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cfca8602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing with: TCS\n",
      "Fetching: https://www.ambitionbox.com/overview/tcs-overview\n",
      "✓ Successfully fetched company details\n"
     ]
    }
   ],
   "source": [
    "# Let's get detailed info for the first company to test\n",
    "test_company = companies_page1[0]['company_name']\n",
    "print(f\"Testing with: {test_company}\")\n",
    "\n",
    "# Convert company name to URL format\n",
    "# Example: \"TCS\" -> \"tcs-overview\"\n",
    "company_slug = test_company.lower().replace(\" \", \"-\") + \"-overview\"\n",
    "detail_url = f\"https://www.ambitionbox.com/overview/{company_slug}\"\n",
    "\n",
    "print(f\"Fetching: {detail_url}\")\n",
    "\n",
    "# Get the company detail page\n",
    "time.sleep(2)  # Wait 2 seconds to be polite\n",
    "detail_response = requests.get(detail_url, headers=headers)\n",
    "\n",
    "if detail_response.status_code == 200:\n",
    "    print(\"✓ Successfully fetched company details\")\n",
    "    detail_html = detail_response.text\n",
    "else:\n",
    "    print(f\"✗ Failed. Status: {detail_response.status_code}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "288955ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 labels and 3 values\n",
      "Founded in: 1968 (57 yrs old)\n",
      "India Employee Count: 1 Lakh+\n",
      "Global Employee Count: 1 Lakh+\n",
      "\n",
      "✓ Extracted company details successfully\n"
     ]
    }
   ],
   "source": [
    "# Parse the detail page and extract information\n",
    "detail_soup = BeautifulSoup(detail_html, 'lxml')\n",
    "\n",
    "# Find all the data spans (these contain the values)\n",
    "data_spans = detail_soup.find_all('span', class_='css-1jxf684 text-primary-text text-sm font-pn-600 flex-[6] md:flex-[auto]')\n",
    "\n",
    "# Find all the labels\n",
    "data_labels = detail_soup.find_all('span', class_='css-1jxf684 text-neutral-300 font-pn-600 text-sm tracking-[0.25px] min-w-[40%] flex-[5]')\n",
    "\n",
    "print(f\"Found {len(data_labels)} labels and {len(data_spans)} values\")\n",
    "\n",
    "# Initialize company details\n",
    "company_details = {\n",
    "    'company_name': test_company,\n",
    "    'rating': companies_page1[0]['rating'],\n",
    "    'founded_year': 'N/A',\n",
    "    'india_employee_count': 'N/A',\n",
    "    'global_employee_count': 'N/A',\n",
    "    'india_headquarters': 'N/A',\n",
    "    'global_headquarters': 'N/A',\n",
    "    'website': 'N/A',\n",
    "    'primary_industry': 'N/A'\n",
    "}\n",
    "\n",
    "# Match labels with values\n",
    "for idx, label_elem in enumerate(data_labels):\n",
    "    label = label_elem.get_text(strip=True)\n",
    "    value = data_spans[idx].get_text(strip=True) if idx < len(data_spans) else 'N/A'\n",
    "    \n",
    "    print(f\"{label}: {value}\")\n",
    "    \n",
    "    if 'Founded' in label:\n",
    "        company_details['founded_year'] = value\n",
    "    elif 'India Employee Count' in label:\n",
    "        company_details['india_employee_count'] = value\n",
    "    elif 'Global Employee Count' in label:\n",
    "        company_details['global_employee_count'] = value\n",
    "    elif 'India Headquarters' in label:\n",
    "        company_details['india_headquarters'] = value\n",
    "    elif 'Website' in label:\n",
    "        company_details['website'] = value\n",
    "    elif 'Primary Industry' in label:\n",
    "        company_details['primary_industry'] = value\n",
    "\n",
    "print(\"\\n✓ Extracted company details successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "201596d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found Global HQ: Mumbai, Maharashtra, India\n",
      "\n",
      "--- Final Company Details ---\n",
      "company_name: TCS\n",
      "rating: 3.4\n",
      "founded_year: 1968 (57 yrs old)\n",
      "india_employee_count: 1 Lakh+\n",
      "global_employee_count: 1 Lakh+\n",
      "india_headquarters: N/A\n",
      "global_headquarters: Mumbai, Maharashtra, India\n",
      "website: N/A\n",
      "primary_industry: N/A\n"
     ]
    }
   ],
   "source": [
    "# Look for office location links\n",
    "hq_links = detail_soup.find_all('a', href=True)\n",
    "\n",
    "for link in hq_links:\n",
    "    if 'offices' in link.get('href', ''):\n",
    "        link_text = link.get_text(strip=True)\n",
    "        # Check if this location is different from India HQ\n",
    "        if link_text != company_details['india_headquarters'] and ',' in link_text:\n",
    "            company_details['global_headquarters'] = link_text\n",
    "            print(f\"Found Global HQ: {link_text}\")\n",
    "            break\n",
    "\n",
    "print(\"\\n--- Final Company Details ---\")\n",
    "for key, value in company_details.items():\n",
    "    print(f\"{key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "80b23da8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting details for first 5 companies...\n",
      "============================================================\n",
      "\n",
      "[1/5] Processing: TCS\n",
      "  ✓ Successfully extracted details\n",
      "\n",
      "[2/5] Processing: Accenture\n",
      "  ✓ Successfully extracted details\n",
      "\n",
      "[3/5] Processing: Wipro\n",
      "  ✓ Successfully extracted details\n",
      "\n",
      "[4/5] Processing: Cognizant\n",
      "  ✓ Successfully extracted details\n",
      "\n",
      "[5/5] Processing: Capgemini\n",
      "  ✓ Successfully extracted details\n",
      "\n",
      "============================================================\n",
      "✓ Collected details for 5 companies\n"
     ]
    }
   ],
   "source": [
    "# Now let's get details for first 5 companies\n",
    "print(\"Getting details for first 5 companies...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "detailed_companies = []\n",
    "\n",
    "for idx, company in enumerate(companies_page1[:5], 1):\n",
    "    print(f\"\\n[{idx}/5] Processing: {company['company_name']}\")\n",
    "    \n",
    "    # Create URL\n",
    "    company_slug = company['company_name'].lower().replace(\" \", \"-\") + \"-overview\"\n",
    "    detail_url = f\"https://www.ambitionbox.com/overview/{company_slug}\"\n",
    "    \n",
    "    try:\n",
    "        # Fetch page\n",
    "        time.sleep(3)  # Wait 3 seconds between requests\n",
    "        detail_response = requests.get(detail_url, headers=headers)\n",
    "        \n",
    "        if detail_response.status_code != 200:\n",
    "            print(f\"  ✗ Failed to fetch. Status: {detail_response.status_code}\")\n",
    "            continue\n",
    "        \n",
    "        # Parse HTML\n",
    "        detail_soup = BeautifulSoup(detail_response.text, 'lxml')\n",
    "        \n",
    "        # Extract data\n",
    "        data_spans = detail_soup.find_all('span', class_='css-1jxf684 text-primary-text text-sm font-pn-600 flex-[6] md:flex-[auto]')\n",
    "        data_labels = detail_soup.find_all('span', class_='css-1jxf684 text-neutral-300 font-pn-600 text-sm tracking-[0.25px] min-w-[40%] flex-[5]')\n",
    "        \n",
    "        # Initialize details\n",
    "        details = {\n",
    "            'company_name': company['company_name'],\n",
    "            'rating': company['rating'],\n",
    "            'founded_year': 'N/A',\n",
    "            'india_employee_count': 'N/A',\n",
    "            'global_employee_count': 'N/A',\n",
    "            'india_headquarters': 'N/A',\n",
    "            'global_headquarters': 'N/A',\n",
    "            'website': 'N/A',\n",
    "            'primary_industry': 'N/A'\n",
    "        }\n",
    "        \n",
    "        # Match labels with values\n",
    "        for i, label_elem in enumerate(data_labels):\n",
    "            label = label_elem.get_text(strip=True)\n",
    "            value = data_spans[i].get_text(strip=True) if i < len(data_spans) else 'N/A'\n",
    "            \n",
    "            if 'Founded' in label:\n",
    "                details['founded_year'] = value\n",
    "            elif 'India Employee Count' in label:\n",
    "                details['india_employee_count'] = value\n",
    "            elif 'Global Employee Count' in label:\n",
    "                details['global_employee_count'] = value\n",
    "            elif 'India Headquarters' in label:\n",
    "                details['india_headquarters'] = value\n",
    "            elif 'Website' in label:\n",
    "                details['website'] = value\n",
    "            elif 'Primary Industry' in label:\n",
    "                details['primary_industry'] = value\n",
    "        \n",
    "        # Get global headquarters\n",
    "        hq_links = detail_soup.find_all('a', href=True)\n",
    "        for link in hq_links:\n",
    "            if 'offices' in link.get('href', ''):\n",
    "                link_text = link.get_text(strip=True)\n",
    "                if link_text != details['india_headquarters'] and ',' in link_text:\n",
    "                    details['global_headquarters'] = link_text\n",
    "                    break\n",
    "        \n",
    "        detailed_companies.append(details)\n",
    "        print(f\"  ✓ Successfully extracted details\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error: {e}\")\n",
    "        continue\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ Collected details for {len(detailed_companies)} companies\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8cbdbb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping pages 1 to 3...\n",
      "============================================================\n",
      "\n",
      "Fetching page 1...\n",
      "  ✓ Found 20 companies on page 1\n",
      "\n",
      "Fetching page 2...\n",
      "  ✓ Found 20 companies on page 2\n",
      "\n",
      "Fetching page 3...\n",
      "  ✓ Found 20 companies on page 3\n",
      "\n",
      "============================================================\n",
      "✓ Total companies collected: 60\n"
     ]
    }
   ],
   "source": [
    "# Let's get companies from multiple pages\n",
    "print(\"Scraping pages 1 to 3...\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "all_companies = []\n",
    "\n",
    "for page_num in range(1, 4):  # Pages 1, 2, 3\n",
    "    print(f\"\\nFetching page {page_num}...\")\n",
    "    \n",
    "    url = f'https://www.ambitionbox.com/list-of-companies?page={page_num}'\n",
    "    response = requests.get(url, headers=headers)\n",
    "    \n",
    "    if response.status_code != 200:\n",
    "        print(f\"  ✗ Failed to fetch page {page_num}\")\n",
    "        continue\n",
    "    \n",
    "    # Parse page\n",
    "    soup = BeautifulSoup(response.text, 'lxml')\n",
    "    \n",
    "    # Extract companies\n",
    "    company_cards = soup.find_all('h2', class_='companyCardWrapper__companyName')\n",
    "    ratings = soup.find_all('div', class_='rating_text')\n",
    "    \n",
    "    for idx, card in enumerate(company_cards):\n",
    "        company_name = card.get_text(strip=True)\n",
    "        company_rating = ratings[idx].text.strip() if idx < len(ratings) else 'N/A'\n",
    "        \n",
    "        all_companies.append({\n",
    "            'company_name': company_name,\n",
    "            'rating': company_rating\n",
    "        })\n",
    "    \n",
    "    print(f\"  ✓ Found {len(company_cards)} companies on page {page_num}\")\n",
    "    time.sleep(2)  # Wait between pages\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ Total companies collected: {len(all_companies)}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "115f3d92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting detailed information...\n",
      "Processing first 10 companies (change this number as needed)\n",
      "============================================================\n",
      "\n",
      "[1/10] TCS\n",
      "  ✓ Success\n",
      "\n",
      "[2/10] Accenture\n",
      "  ✓ Success\n",
      "\n",
      "[3/10] Wipro\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 17\u001b[39m\n\u001b[32m     13\u001b[39m detail_url = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mhttps://www.ambitionbox.com/overview/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompany_slug\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     15\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     16\u001b[39m     \u001b[38;5;66;03m# Fetch\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m17\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     18\u001b[39m     detail_response = requests.get(detail_url, headers=headers)\n\u001b[32m     20\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m detail_response.status_code != \u001b[32m200\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# This will take time, so we limit to first 10 companies\n",
    "print(\"Getting detailed information...\")\n",
    "print(\"Processing first 10 companies (change this number as needed)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "final_detailed_data = []\n",
    "\n",
    "for idx, company in enumerate(all_companies[:60], 1):  # Change [:10] to [:None] for all\n",
    "    print(f\"\\n[{idx}/10] {company['company_name']}\")\n",
    "    \n",
    "    # Create URL slug\n",
    "    company_slug = company['company_name'].lower().replace(\" \", \"-\") + \"-overview\"\n",
    "    detail_url = f\"https://www.ambitionbox.com/overview/{company_slug}\"\n",
    "    \n",
    "    try:\n",
    "        # Fetch\n",
    "        time.sleep(3)\n",
    "        detail_response = requests.get(detail_url, headers=headers)\n",
    "        \n",
    "        if detail_response.status_code != 200:\n",
    "            print(f\"  ✗ Failed (Status: {detail_response.status_code})\")\n",
    "            continue\n",
    "        \n",
    "        # Parse\n",
    "        detail_soup = BeautifulSoup(detail_response.text, 'lxml')\n",
    "        data_spans = detail_soup.find_all('span', class_='css-1jxf684 text-primary-text text-sm font-pn-600 flex-[6] md:flex-[auto]')\n",
    "        data_labels = detail_soup.find_all('span', class_='css-1jxf684 text-neutral-300 font-pn-600 text-sm tracking-[0.25px] min-w-[40%] flex-[5]')\n",
    "        \n",
    "        # Store details\n",
    "        details = {\n",
    "            'company_name': company['company_name'],\n",
    "            'rating': company['rating'],\n",
    "            'founded_year': 'N/A',\n",
    "            'india_employee_count': 'N/A',\n",
    "            'global_employee_count': 'N/A',\n",
    "            'india_headquarters': 'N/A',\n",
    "            'global_headquarters': 'N/A',\n",
    "            'website': 'N/A',\n",
    "            'primary_industry': 'N/A'\n",
    "        }\n",
    "        \n",
    "        # Extract details\n",
    "        for i, label_elem in enumerate(data_labels):\n",
    "            label = label_elem.get_text(strip=True)\n",
    "            value = data_spans[i].get_text(strip=True) if i < len(data_spans) else 'N/A'\n",
    "            \n",
    "            if 'Founded' in label:\n",
    "                details['founded_year'] = value\n",
    "            elif 'India Employee Count' in label:\n",
    "                details['india_employee_count'] = value\n",
    "            elif 'Global Employee Count' in label:\n",
    "                details['global_employee_count'] = value\n",
    "            elif 'India Headquarters' in label:\n",
    "                details['india_headquarters'] = value\n",
    "            elif 'Website' in label:\n",
    "                details['website'] = value\n",
    "            elif 'Primary Industry' in label:\n",
    "                details['primary_industry'] = value\n",
    "        \n",
    "        # Global HQ\n",
    "        hq_links = detail_soup.find_all('a', href=True)\n",
    "        for link in hq_links:\n",
    "            if 'offices' in link.get('href', ''):\n",
    "                link_text = link.get_text(strip=True)\n",
    "                if link_text != details['india_headquarters'] and ',' in link_text:\n",
    "                    details['global_headquarters'] = link_text\n",
    "                    break\n",
    "        \n",
    "        final_detailed_data.append(details)\n",
    "        print(f\"  ✓ Success\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ✗ Error: {e}\")\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ Total detailed records: {len(final_detailed_data)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae310601",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Preview:\n",
      "============================================================\n",
      "    company_name rating       founded_year india_employee_count  \\\n",
      "0            TCS    3.4  1968 (57 yrs old)              1 Lakh+   \n",
      "1      Accenture    3.7  1989 (36 yrs old)              1 Lakh+   \n",
      "2          Wipro    3.6  1945 (80 yrs old)              1 Lakh+   \n",
      "3      Cognizant    3.7  1994 (31 yrs old)              1 Lakh+   \n",
      "4      Capgemini    3.7  1967 (58 yrs old)              1 Lakh+   \n",
      "5      HDFC Bank    3.8  1994 (31 yrs old)              1 Lakh+   \n",
      "6        Infosys    3.5                N/A                  N/A   \n",
      "7     ICICI Bank    4.0  1994 (31 yrs old)              1 Lakh+   \n",
      "8        HCLTech    3.4                N/A                  N/A   \n",
      "9  Tech Mahindra    3.4  1986 (39 yrs old)              1 Lakh+   \n",
      "\n",
      "  global_employee_count india_headquarters  \\\n",
      "0               1 Lakh+                N/A   \n",
      "1               1 Lakh+                N/A   \n",
      "2               1 Lakh+                N/A   \n",
      "3               1 Lakh+                N/A   \n",
      "4               1 Lakh+                N/A   \n",
      "5               1 Lakh+                N/A   \n",
      "6                   N/A                N/A   \n",
      "7               1 Lakh+                N/A   \n",
      "8                   N/A                N/A   \n",
      "9               1 Lakh+                N/A   \n",
      "\n",
      "                                 global_headquarters website primary_industry  \n",
      "0                         Mumbai, Maharashtra, India     N/A              N/A  \n",
      "1  Bangalore / Bengaluru, Karnataka3.61 office lo...     N/A              N/A  \n",
      "2              Bangalore/Bengaluru, Karnataka, India     N/A              N/A  \n",
      "3  Hyderabad / Secunderabad, Telangana3.71 office...     N/A              N/A  \n",
      "4                             Pune, Maharashtra3.7HQ     N/A              N/A  \n",
      "5                         Mumbai, Maharashtra, India     N/A              N/A  \n",
      "6  Bangalore / Bengaluru, Karnataka3.4HQ& 7 offic...     N/A              N/A  \n",
      "7                         Mumbai, Maharashtra, India     N/A              N/A  \n",
      "8                               Noida, Uttar Pradesh     N/A              N/A  \n",
      "9                           Pune, Maharashtra, India     N/A              N/A  \n",
      "\n",
      "============================================================\n",
      "Shape: (10, 9)\n",
      "Columns: ['company_name', 'rating', 'founded_year', 'india_employee_count', 'global_employee_count', 'india_headquarters', 'global_headquarters', 'website', 'primary_industry']\n",
      "✓ Data saved to: ambitionbox_companies.csv\n",
      "✓ Total records saved: 10\n",
      "============================================================\n",
      "DATA SUMMARY\n",
      "============================================================\n",
      "\n",
      "Total Companies: 10\n",
      "\n",
      "Column Names:\n",
      "  - company_name\n",
      "  - rating\n",
      "  - founded_year\n",
      "  - india_employee_count\n",
      "  - global_employee_count\n",
      "  - india_headquarters\n",
      "  - global_headquarters\n",
      "  - website\n",
      "  - primary_industry\n",
      "\n",
      "Missing Values:\n",
      "company_name             0\n",
      "rating                   0\n",
      "founded_year             0\n",
      "india_employee_count     0\n",
      "global_employee_count    0\n",
      "india_headquarters       0\n",
      "global_headquarters      0\n",
      "website                  0\n",
      "primary_industry         0\n",
      "dtype: int64\n",
      "\n",
      "Rating Distribution:\n",
      "rating\n",
      "3.4    3\n",
      "3.7    3\n",
      "3.6    1\n",
      "3.8    1\n",
      "3.5    1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Convert our data to a pandas DataFrame\n",
    "df = pd.DataFrame(final_detailed_data)\n",
    "\n",
    "print(\"Data Preview:\")\n",
    "print(\"=\"*60)\n",
    "print(df.head(10))\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "\n",
    "\n",
    "# Cell 12: Save to CSV File\n",
    "# Save our scraped data to a CSV file\n",
    "filename = 'ambitionbox_companies.csv'\n",
    "df.to_csv(filename, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"✓ Data saved to: {filename}\")\n",
    "print(f\"✓ Total records saved: {len(df)}\")\n",
    "\n",
    "\n",
    "# Cell 13: View Summary Statistics\n",
    "print(\"=\"*60)\n",
    "print(\"DATA SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\"\\nTotal Companies: {len(df)}\")\n",
    "print(f\"\\nColumn Names:\")\n",
    "for col in df.columns:\n",
    "    print(f\"  - {col}\")\n",
    "\n",
    "print(f\"\\nMissing Values:\")\n",
    "print(df.isnull().sum())\n",
    "\n",
    "print(f\"\\nRating Distribution:\")\n",
    "print(df['rating'].value_counts().head())\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
